## LLM as a judge  

## Required
- User Input
- LLM Output
- Retrieval Context

## 描述
**忠實性**指標用於衡量 RAG 流程中生成的回答與檢索上下文內容的事實一致性。此指標基於生成回應中是否存在矛盾，從而判斷模型回答是否忠實於檢索上下文。**FaithfulnessMetric** 是一種自解釋 LLM 評估方式，會為其分數提供詳細的理由說明。

忠實性分數會比對生成回應中的各個陳述，並檢查是否能從檢索上下文中推導出或不矛盾。如果一個陳述與上下文一致，即視為忠實。

## 計算方法
忠實性分數的計算包含以下步驟：

1. **提取陳述**：使用 LLM 從生成回應中提取所有陳述。
2. **分類忠實性**：使用 LLM 檢查每條陳述是否與檢索上下文內容一致。
3. **忠實性分數公式**：

$$
\text{Faithfulness} = \frac{\text{Number of Truthful Claims}}{\text{Total Number of Claims}}
$$

## 程式碼
```python
from deepeval import evaluate
from deepeval.metrics import FaithfulnessMetric
from deepeval.test_case import LLMTestCase

# 生成回應
actual_output = "We offer a 30-day full refund at no extra cost."

# 檢索上下文
retrieval_context = ["All customers are eligible for a 30 day full refund at no extra cost."]

# 初始化忠實性評分器並設置參數
metric = FaithfulnessMetric(
    threshold=0.7,
    model="gpt-4",
    include_reason=True
)

# 設置測試案例
test_case = LLMTestCase(
    input="What if these shoes don't fit?",
    actual_output=actual_output,
    retrieval_context=retrieval_context
)

# 計算忠實性分數
metric.measure(test_case)
print("Faithfulness Score:", metric.score)
print("Explanation:", metric.reason)
```

在這個範例中，**Faithfulness Score** 反映出生成回應與檢索上下文的一致性程度，`Explanation` 提供分數的解釋以說明一致或矛盾的原因。

## 參考資料:
https://docs.confident-ai.com/docs/metrics-faithfulness