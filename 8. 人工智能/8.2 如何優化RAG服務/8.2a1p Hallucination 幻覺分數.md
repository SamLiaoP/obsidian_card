## LLM as a judge  

## Required
- User Input
- LLM Output
- Retrieval Context

## 描述
**幻覺性**指標用於檢測生成回應中的資訊是否與提供的上下文存在事實矛盾。當生成的回應中出現無法從上下文中推導出的訊息時，即視為「幻覺」。**HallucinationMetric** 主要用於檢查回應是否包含與上下文不一致的訊息，並會提供評估分數的詳細說明。

## 計算方法
幻覺性分數的計算包含以下步驟：

1. **檢查矛盾**：使用 LLM 逐一檢查每個上下文，確定是否有任何與生成回應不一致的內容。
2. **幻覺性分數公式**：

$$
\text{Hallucination} = \frac{\text{Number of Contradicted Contexts}}{\text{Total Number of Contexts}}
$$

若所有上下文均無矛盾，分數接近 0；若回應在多數上下文中有矛盾，分數接近 1。

## 程式碼
```python
from deepeval import evaluate
from deepeval.metrics import HallucinationMetric
from deepeval.test_case import LLMTestCase

# 檢索到的上下文
context=["A man with blond-hair, and a brown shirt drinking out of a public water fountain."]

# 生成的回應
actual_output="A blond drinking water in public."

# 設定測試案例
test_case = LLMTestCase(
    input="What was the blond doing?",
    actual_output=actual_output,
    context=context
)

# 初始化幻覺性評分器並設置閾值
metric = HallucinationMetric(threshold=0.5)

# 計算幻覺性分數
metric.measure(test_case)
print("Hallucination Score:", metric.score)
print("Explanation:", metric.reason)
```

範例輸出可能會顯示幻覺性分數（例如 `0.0` 或 `1.0`），依據回應中的內容是否與上下文一致來計算。`Explanation` 提供了分數的解釋，以說明矛盾的來源。