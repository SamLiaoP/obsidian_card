
## Required
- User Input
- Ground True
- Retrieved Contexts

## 描述
**上下文召回率**測量檢索到的上下文中包含多少相關的資訊，並重點檢查是否遺漏了重要的內容。高召回率表示檢索系統成功包含了更多相關文件或資訊。**Context Recall** 的計算需要有參考資訊作為比較基準，並以此確保重要內容未被遺漏。

### 計算方法
上下文召回率依據以下公式計算：

$$
\text{Context Recall} = \frac{\text{Number of Relevant Claims in Retrieved Contexts}}{\text{Total Number of Claims in Reference}}
$$

1. **LLM 基於模型的上下文召回率**：將 **Reference** 拆解為多個陳述，並逐一比對檢索上下文，確認是否每個陳述都能從檢索上下文推導出來。
2. **非 LLM 基於模型的上下文召回率**：使用傳統字串比對方法來判斷檢索上下文是否包含與參考上下文一致的內容。

## 程式碼

### 1. LLMContextRecall
```python
from ragas.dataset_schema import SingleTurnSample
from ragas.metrics import LLMContextRecall

# 準備測試數據
sample = SingleTurnSample(
    user_input="Where is the Eiffel Tower located?",
    response="The Eiffel Tower is located in Paris.",
    reference="The Eiffel Tower is located in Paris.",
    retrieved_contexts=["Paris is the capital of France."]
)

# 初始化上下文召回率計分器
context_recall = LLMContextRecall()
await context_recall.single_turn_ascore(sample)
```

### 2. NonLLMContextRecall
```python
from ragas.dataset_schema import SingleTurnSample
from ragas.metrics import NonLLMContextRecall

# 準備測試數據
sample = SingleTurnSample(
    retrieved_contexts=["Paris is the capital of France."],
    reference_contexts=["Paris is the capital of France.", "The Eiffel Tower is one of the most famous landmarks in Paris."]
)

# 初始化上下文召回率計分器
context_recall = NonLLMContextRecall()
await context_recall.single_turn_ascore(sample)
```

在此範例中，**Context Recall** 分數反映了檢索上下文的完整性。高分數表示檢索結果更全面地涵蓋了參考內容。