{
  "file_id": "b4e9092a",
  "name": "KubeLLM.md",
  "path": "4. 軟體開發/GCP/KubeLLM.md",
  "content": "以下是一個 **更完整、包含「新增與修改檔案指令」** 的教學流程範本，讓你在 **GCP 上部署 KubeAI** 以及 **LLama3.2 3B** 模型，並且包含 **如何新增/修改檔案** 的示例命令。整個流程適用於 **Regional GKE Cluster** 並使用 **T4 GPU**。如有必要你可以將特定檔名、內容根據實際需求調整。\n\n---\n\n## **整體步驟總覽**\n\n1. 建立並設定 GKE Cluster\n2. 新增 GPU Node Pool (並鎖定到支援 T4 的子區域)\n3. 安裝 KubeAI Controller (包含下載與修改 `values-gke.yaml`)\n4. 新增並部署模型目錄 (`kubeai-models.yaml`)\n5. 新增並部署 LLama3.2 3B 模型 (`llama3-3b.yaml`)\n6. 測試模型 (OpenWebUI 與 Port-Forward 至 Jupyter Notebook)\n\n---\n\n## **步驟 1：建立並設定 GKE Cluster**\n\n1. **更新 gcloud CLI**\n    \n    ```bash\n    gcloud components update\n    ```\n    \n2. **建立 Regional GKE Cluster**\n    \n    ```bash\n    gcloud container clusters create kubeai-cluster \\\n        --region asia-east1 \\\n        --release-channel regular \\\n        --logging=SYSTEM,WORKLOAD \\\n        --monitoring=SYSTEM \\\n        --enable-autoscaling \\\n        --num-nodes=1 \\\n        --min-nodes=1 \\\n        --max-nodes=3\n    ```\n    \n3. **連線到該集群**\n    \n    ```bash\n    gcloud container clusters get-credentials kubeai-cluster --region asia-east1\n    ```\n    \n\n---\n\n## **步驟 2：新增 GPU Node Pool**\n\n### **2.1 查詢支援 T4 的區域/zone**\n\n```bash\ngcloud compute accelerator-types list --filter=\"name:nvidia-tesla-t4\"\n```\n\n假設我們確定 `asia-east1-a` 有支援 T4。\n\n### **2.2 建立 GPU Node Pool**\n\n```bash\ngcloud container node-pools create gpu-pool \\\n    --cluster kubeai-cluster \\\n    --region asia-east1 \\\n    --node-locations=asia-east1-a \\\n    --accelerator type=nvidia-tesla-t4,count=1 \\\n    --machine-type n1-highmem-8 \\\n    --num-nodes=1 \\\n    --enable-autoscaling \\\n    --min-nodes=0 \\\n    --max-nodes=2 \\\n    --labels node-role=gpu \\\n    --preemptible\n```\n\n> _`--node-locations=asia-east1-a` 鎖定到有 T4 的子區域，`--preemptible` 表示可搶占節點，成本更低但可能會中斷。_\n\n---\n\n## **步驟 3：安裝 KubeAI Controller**\n\n### **3.1 安裝 Helm 並加入 Repo (若尚未安裝 Helm)**\n\n若你尚未在 Cloud Shell 安裝 Helm，可先安裝：\n\n```bash\ncurl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\n```\n\n加入並更新 KubeAI Repository：\n\n```bash\nhelm repo add kubeai https://substratusai.github.io/kubeai\nhelm repo update\n```\n\n### **3.2 下載並修改 `values-gke.yaml`**\n\n1. **下載檔案**\n    \n    ```bash\n    curl -L -O https://raw.githubusercontent.com/substratusai/kubeai/refs/heads/main/charts/kubeai/values-gke.yaml\n    ```\n    \n2. **編輯檔案**  \n    你可用任一文字編輯器（`nano`, `vi` 等）開啟並修改；以下以 `nano` 為例：\n    \n    ```bash\n    nano values-gke.yaml\n    ```\n    \n    在 `resourceProfiles` 區塊中，若沒有 T4 配置，可手動新增：\n    \n    ```yaml\n    resourceProfiles:\n      nvidia-gpu-t4:\n        nodeSelector:\n          cloud.google.com/gke-accelerator: \"nvidia-tesla-t4\"\n          cloud.google.com/gke-spot: \"true\"\n    ```\n    \n    編輯完按 `Ctrl+X`, `Y`, `Enter` 儲存退出。\n    \n\n### **3.3 部署 KubeAI Controller**\n\n1. **設置 Hugging Face Token（若需要下載 HF 模型）**\n    \n    ```bash\n    export HUGGING_FACE_HUB_TOKEN=[Your-HF-Access-Token]\n    ```\n    \n2. **使用 Helm 部署**\n    \n    ```bash\n    helm upgrade --install kubeai kubeai/kubeai \\\n        -f values-gke.yaml \\\n        --set secrets.huggingface.token=$HUGGING_FACE_HUB_TOKEN \\\n        --wait\n    ```\n    \n3. **確認 Pods**\n    \n    ```bash\n    kubectl get pods -n kubeai\n    ```\n    \n    確認 `kubeai` 和 `openwebui` 等相關 Pod 已正確啟動。\n\n---\n\n## **步驟 4：新增並部署模型目錄 (`kubeai-models.yaml`)**\n\n1. **新增檔案**  \n    若要在 Shell 中一次完成檔案建立，可使用這種 `cat <<EOF` 語法：\n    \n```bash\ncat <<EOF > kubeai-models.yaml\ncatalog:\n  llama3-3b:\n    enable: true\nEOF\n````\n\n```bash\nhelm install kubeai-models kubeai/models -f kubeai-models.yaml\n```\n\n\n---\n\n## **步驟 5：新增並部署 LLama3.2 3B 模型 (`llama3-3b.yaml`)**\n\n1. **新增檔案**\n    \n```bash\ncat <<EOF > llama3-3b.yaml\napiVersion: kubeai.org/v1\nkind: Model\nmetadata:\n  name: llama3-3b\nspec:\n  features: [TextGeneration]\n  owner: openai\n  url: ollama://llama3:3b\n  engine: OLlama\n  resourceProfile: nvidia-gpu-t4:1\n  minReplicas: 0\n  replicas: 1\nEOF\n````\n> *`resourceProfile: nvidia-gpu-t4:1` 表示使用剛才在 `values-gke.yaml` 定義的 T4 profile 並分配 1 顆 GPU。*\n\n2. **套用該模型檔案**  \n```bash\nkubectl apply -f llama3-3b.yaml\n````\n\n3. **檢查狀態**\n    \n    ```bash\n    kubectl get model llama3-3b\n    kubectl get pods\n    ```\n    \n    確認模型對應的 Pod 順利啟動。\n\n---\n\n## **步驟 6：測試模型**\n\n### **6.1 透過 OpenWebUI 測試**\n\n1. **Port-Forward**\n    \n    ```bash\n    kubectl port-forward svc/openwebui 8080:80\n    ```\n    \n2. **在瀏覽器中打開**  \n    `http://localhost:8080`  \n    選擇 `llama3-3b` 進行交互測試。\n\n### **6.2 讓 Jupyter Notebook 呼叫模型**\n\n#### **6.2.1 Port-Forward 方式**\n\n1. **開啟另一個終端**\n    \n    ```bash\n    kubectl port-forward svc/kubeai 8080:80\n    ```\n    \n2. **Notebook 內測試** (範例)\n    \n    ```python\n    import requests\n    \n    url = \"http://localhost:8080/v1/chat/completions\"\n    payload = {\n        \"model\": \"llama3-3b\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n        ]\n    }\n    headers = {\"Content-Type\": \"application/json\"}\n    \n    response = requests.post(url, json=payload, headers=headers)\n    print(response.json())\n    ```\n    \n\n#### **6.2.2 若 Notebook 與 K8s 同網路 (Cluster IP)**\n\n- 可以建立一個 `ClusterIP` or `LoadBalancer` Service 來讓 Notebook 直接用 `ClusterIP` / `LoadBalancer IP` 去存取。\n- 例如，建立一個 `Service` 針對我們的模型 Pod Selector，也或直接存取 `svc/kubeai` 之 `ClusterIP`。\n\n---\n\n## **總結建議**\n\n1. **Node Pool 自動擴縮**\n    \n    - 由於我們設定 `--min-nodes=0 --max-nodes=2`，在負載低的時候，GPU 節點可以自動縮容至 0，節省成本。\n2. **Spot/Preemptible 節點**\n    \n    - 若對可搶占的中斷風險容忍度高，可用 `--preemptible` 節省更多費用。\n3. **Scale 你的模型**\n    \n    - 用 `kubectl scale model/llama3-3b --replicas=2` 可快速擴充 Pod 副本數； 或 `--replicas=0` 完全關閉以節省 GPU。\n4. **檢查 Logs 與 Metrics**\n    \n    - `kubectl logs` 及 `kubectl top pods/nodes` 監控使用率，確定部署健康度。\n\n---\n\n以上流程已包含 **如何新增與修改檔案** 的指令（使用 `nano` 或 `cat <<EOF > file.yaml`），以及 **如何在 Cloud Shell 進行操作**。照此步驟應能順利在 **GCP（Regional GKE + T4 GPU）** 上啟用 **KubeAI** 以及部署 **LLama3.2 3B** 模型，再透過 **Notebook** 呼叫。若有進一步疑問，歡迎再討論！",
  "size": 5709,
  "lines": 275
}