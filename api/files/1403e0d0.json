{
  "file_id": "1403e0d0",
  "name": "GKE 部署 LLM (Kserve).md",
  "path": "4. 軟體開發/GCP/GKE 部署 LLM (Kserve).md",
  "content": "## 1. 部署GKE\n\n```bash\ngcloud container clusters create sam-llm-test-cluster \\\n    --zone asia-east1-a \\\n    --machine-type n1-highmem-8 \\\n    --accelerator type=nvidia-tesla-t4,count=1 \\\n    --num-nodes 2\n```\n\n```bash\ngcloud container clusters get-credentials sam-llm-test-cluster --zone asia-east1-a\n\n```\n\n## 2. 安裝 Kserve\n```bash\n# Knative 和 KServe 安裝\n# Apply Knative Serving CRDs\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.2/serving-crds.yaml\n\n# Apply Knative Serving Core\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.2/serving-core.yaml\n\n# Wait for Knative Serving components to be ready\nkubectl wait --for=condition=available --timeout=600s deployment/controller -n knative-serving\nkubectl wait --for=condition=available --timeout=600s deployment/webhook -n knative-serving\n\n# Apply Istio\nkubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.15.1/istio.yaml\nkubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.15.1/net-istio.yaml\n\n# Wait until Istio resources are ready\nkubectl wait --for=condition=available --timeout=600s deployment/istio-ingressgateway -n istio-system\nkubectl wait --for=condition=available --timeout=600s deployment/istiod -n istio-system\n\n# Scale down Istio components\nkubectl scale deployment istio-ingressgateway --replicas=1 -n istio-system\nkubectl scale deployment istiod --replicas=1 -n istio-system\n\n# Verify that Ingress Gateway service is available\nkubectl --namespace istio-system get service istio-ingressgateway\n\n# Apply Knative default configurations\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.2/serving-default-domain.yaml\nkubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.15.2/serving-hpa.yaml\n\n# Apply Cert-Manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.0/cert-manager.yaml\n\n# Wait for Cert-Manager components to be ready\nkubectl wait --for=condition=available --timeout=600s deployment/cert-manager -n cert-manager\nkubectl wait --for=condition=available --timeout=600s deployment/cert-manager-webhook -n cert-manager\nkubectl wait --for=condition=available --timeout=600s deployment/cert-manager-cainjector -n cert-manager\n\n# Apply KServe\nkubectl apply -f https://github.com/kserve/kserve/releases/download/v0.13.0/kserve.yaml\n\n# Wait for KServe components to be ready\nkubectl wait --for=condition=available --timeout=600s deployment/kserve-controller-manager -n kserve\nkubectl wait --for=condition=available --timeout=600s deployment/kserve-webhook-server -n kserve\n\n# Apply KServe Built-in ClusterServingRuntime\nkubectl apply -f https://github.com/kserve/kserve/releases/download/v0.13.0/kserve-cluster-resources.yaml\n\necho \"All resources have been applied successfully!\"\n\n```\n\n\n驗證安裝\n```bash\nkubectl get pods -n knative-serving\n\n```\n## 3. 部署LLM\n\n創建命名空間\n```bash\nkubectl create namespace sam-llm-test-namespace\n```\n\n可以看資源\n```bash\nkubectl describe nodes | grep -E \"nvidia.com/gpu|memory\"\n\n```\n\n更新 kserve-huggingfaceserver\n```bash\ncat <<EOF > updated-huggingface-runtime.yaml\napiVersion: serving.kserve.io/v1alpha1\nkind: ClusterServingRuntime\nmetadata:\n  name: kserve-huggingfaceserver\nspec:\n  annotations:\n    prometheus.kserve.io/path: /metrics\n    prometheus.kserve.io/port: \"8080\"\n  containers:\n  - args:\n    - --model_name={{.Name}}\n    image: kserve/huggingfaceserver:latest-gpu\n    name: kserve-container\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: 2Gi\n        nvidia.com/gpu: \"1\"\n      requests:\n        cpu: \"1\"\n        memory: 2Gi\n        nvidia.com/gpu: \"1\"\n  protocolVersions:\n  - v2\n  - v1\n  supportedModelFormats:\n  - autoSelect: true\n    name: huggingface\n    priority: 1\n    version: \"1\"\nEOF\n\n```\n\n```bash\nkubectl apply -f updated-huggingface-runtime.yaml\n\n```\n\n如果成功會看到\n```\nclusterservingruntime.serving.kserve.io/kserve-huggingfaceserver configured\n\n```\n\n\n\nsam-llm-test-llama.yaml\n```bash\n\ncat <<EOF > sam-llm-test-llama.yaml\napiVersion: serving.kserve.io/v1beta1\nkind: InferenceService\nmetadata:\n  name: sam-llm-test-llama\n  namespace: sam-llm-test-namespace\n  annotations:\n    autoscaling.knative.dev/target: \"1\"\nspec:\n  predictor:\n    model:\n      modelFormat:\n        name: huggingface\n      args:\n        - --model_name=Llama3.2-3B\n        - --model_id=meta-llama/Llama-3.2-3B-Instruct\n        - --rope_scaling={\"type\":\"linear\", \"factor\":32.0}\n        - --task=auto\n        - --force_download=True\n        - --gpu_memory_utilization=0.8\n        - --max_model_len=8192\n      env:\n        - name: HF_TOKEN\n          value: \"\"\n      resources:\n        limits:\n          cpu: \"4\"\n          memory: 20Gi\n          nvidia.com/gpu: \"1\"  # 減少 GPU 請求為 1\n        requests:\n          cpu: \"4\"\n          memory: 20Gi\n          nvidia.com/gpu: \"1\"  # 減少 GPU 請求為 1\n    minReplicas: 1\nEOF\n\n\n\n```\n\n\n部署模型\n```bash\nkubectl apply -f sam-llm-test-llama.yaml\n```\n\n驗證模型部署\n```bash\nkubectl get inferenceservices sam-llm-test-llama -n sam-llm-test-namespace\n```\n\n```bash\nkubectl describe inferenceservice sam-llm-test-llama -n sam-llm-test-namespace\n```\n## 4. 測試連線\n```bash\nkubectl get InferenceService -n sam-llm-test-namespace\n\n```\n\n```bash\nkubectl get inferenceservices sam-llm-test-qwen -n sam-llm-test-namespace\nkubectl get pods -n sam-llm-test-namespace\nkubectl get svc istio-ingressgateway -n istio-system\n\n```\n\n## 5. 刪除\n```bash\nkubectl delete inferenceservice sam-llm-test-llama -n sam-llm-test-namespace\n```\n\n```bash\nkubectl delete namespace sam-llm-test-namespace\n```\n",
  "size": 5589,
  "lines": 217
}